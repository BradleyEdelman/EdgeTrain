{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a7a2f4-4683-4ae2-908a-57f4a079d77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3096e15-5b6c-412d-9342-33e640c7c733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Workspace/Users/bjedelma@gmail.com/ScaleML/scaleml')\n",
    "import scaleml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cce1389-9ae2-413a-a9ff-bf59e57e47c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Detect computational resources on current node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e43cf733-8786-4f8b-8c4b-1adaa5e37333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detect computing resources\n",
    "detected_resources = scaleml.resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b221868-15b5-487b-a65b-b17a8b9e214c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify ML frameworks for the model and distributed strategy\n",
    "framework = {\n",
    "    \"model\": \"tensorflow\",\n",
    "    \"strategy\": \"tensorflow\",\n",
    "}\n",
    "\n",
    "# Set the distributed training strategy\n",
    "dist_strategy = scaleml.strategies(framework, detected_resources, resource_type='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805d6dba-0acf-4b10-a813-a85875a9917b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset for example\n",
    "import tensorflow as tf\n",
    "\n",
    "# MNIST as example model and dataset (you can replace with your actual dataset/model)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to a range of 0 to 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Select half the train data\n",
    "half_index = len(train_images) // 8\n",
    "train_images, train_labels = train_images[:half_index], train_labels[:half_index]\n",
    "train_images = train_images.reshape((-1, 28, 28, 1))\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(64)\n",
    "\n",
    "display(train_images, train_labels, test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0efe2723-38bf-4031-a346-f1d32f453d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Train images shape: {train_images.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80818728-0d9d-4b69-8301-1bb28639097f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dist_strategy.scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8b8d9b-4ae3-400d-a51f-84b91b5e50ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train model with distributed strategy\n",
    "history, history_file, log_file = scaleml.distributed_train(framework, dist_strategy, train_dataset, log_resources=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e73845c-6b85-4320-898a-50e72c40630e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def distributed_train(framework, strategy, train_dataset, log_resources=True):\n",
    "strategy = dist_strategy\n",
    "\"\"\"\n",
    "Train a model using a specified framework and strategy, and track resource usage (optional).\n",
    "\n",
    "Parameters:\n",
    "- framework (dict): A dictionary containing the framework details.\n",
    "                        Expected keys: 'model', 'strategy'.\n",
    "                        Options for 'model': 'tensorflow', 'pytorch'.\n",
    "                        Options for 'strategy': 'tensorflow', 'pytorch', 'horovod'.\n",
    "- strategy: distributed ScaleML training strategy appropriate for the model.\n",
    "- train_dataset: Dataset used for training.\n",
    "- log_resources: Boolean flag to enable/disable resource logging.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from multiprocessing import Process, Value\n",
    "import os, time, pickle\n",
    "# import horovod.tensorflow as hvd_tf\n",
    "# import horovod.torch as hvd_torch\n",
    "\n",
    "from scaleml import create_scaleml_folders, create_model, start_logging, stop_logging\n",
    "\n",
    "# create folders for saving\n",
    "scaleml_folder = create_scaleml_folders()\n",
    "print(f\"Scaleml folder and subfolders are set up at: {scaleml_folder}\")\n",
    "print()\n",
    "\n",
    "# log resource use throughout training\n",
    "log_dir = f\"{scaleml_folder}/logs/\"\n",
    "log_file = f\"{log_dir}/{datetime.now().strftime('%Y%m%d')}_resource_usage_log.csv\"\n",
    "# if log_resources:        \n",
    "# Start logging resources in the background (adjust the interval as needed)\n",
    "log_process = start_logging(log_file, interval=10)\n",
    "print()\n",
    "\n",
    "# Detect size of the first image from train_dataset\n",
    "for images, labels in train_dataset.take(1):\n",
    "    input_shape = images.shape[1:]\n",
    "    break\n",
    "\n",
    "# create model based on the detected framework\n",
    "model = create_model(strategy, framework, input_shape)\n",
    "\n",
    "# Save the model\n",
    "log_time = log_file.split('/')[-1]\n",
    "log_time = log_time.split('_')[0]\n",
    "if framework == 'tensorflow':\n",
    "    model.save(os.path.join(f\"{scaleml_folder}/models/\", f\"{log_time}_tf_model.h5\"))\n",
    "elif framework == 'pytorch':\n",
    "    torch.save(model.state_dict(), os.path.join(f\"{scaleml_folder}/models/\", 'f\"{log_time}_torch_model.pth'))\n",
    "\n",
    "# Start the training process with or without resource monitoring\n",
    "if isinstance(strategy, tf.distribute.MirroredStrategy):\n",
    "\n",
    "    with strategy.scope():\n",
    "        history = model.fit(train_dataset, epochs=2)\n",
    "        history_file = os.path.join(f\"{scaleml_folder}/models/\", f\"{log_time}_tf_model_history.pkl\")\n",
    "        with open(history_file, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "\n",
    "elif isinstance(strategy, torch.device):\n",
    "    model.to(strategy)\n",
    "    # history = train_pytorch_model(model, train_dataset, epochs=2)\n",
    "    history_file = os.path.join(f\"{scaleml_folder}/models/\", f\"{log_time}_torch_model_history.pkl\")\n",
    "    with open(history_file, 'wb') as f:\n",
    "            pickle.dump(history.history, f)\n",
    "\n",
    "# elif 'horovod' in str(type(strategy)).lower():\n",
    "\n",
    "# # if log_resources:\n",
    "# # Stop the logging once training is complete\n",
    "# stop_logging(log_process)\n",
    "\n",
    "# print(\"Training completed.\")\n",
    "# return history, history_file, log_file"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "single_node_tensorflow",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
