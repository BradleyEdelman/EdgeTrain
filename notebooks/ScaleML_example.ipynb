{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3205090d-99f7-41cd-8009-074e45b6a12d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Workspace/Users/bjedelma@gmail.com/ScaleML/scaleml')\n",
    "import scaleml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb6017a-aafb-4dca-b981-fe6123ee90fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from scaleml import dynamic_train, log_usage_once, adjust_workers, adjust_batch_size, log_usage_plot, scaleml_folders, dynamic_train, create_model_tf, sys_resources\n",
    "\n",
    "# Load the MNIST dataset from tensorflow\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "test_images = test_images.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "train_dataset = {'images': train_images, 'labels': train_labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f10f2a4c-b32c-46e9-845f-81fe44061601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create folders for saving\n",
    "scaleml_folder = scaleml_folders()\n",
    "print(f\"Scaleml folder and subfolders are set up at: {scaleml_folder}\")\n",
    "print()\n",
    "\n",
    "# log resource use throughout training\n",
    "log_dir = f\"{scaleml_folder}/logs/\"\n",
    "log_file = f\"{log_dir}/{datetime.now().strftime('%Y%m%d_%H%M%S')}_resource_usage_log.csv\"\n",
    "print(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3049c2b2-a009-4355-8759-e48016e2618d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# better way to store resource log file\n",
    "dynamic_train(train_dataset, epochs=20, base_batch_size=32, log_file=log_file, dynamic_adjustments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e67e94a0-e8e1-4020-b72a-a334a42b244f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test with dynamic adjustments on\n",
    "print(\"Testing with dynamic adjustments ON:\")\n",
    "test_dynamic_training_mirrored(dynamic_adjustments=True, epochs=5)\n",
    "\n",
    "# Test with dynamic adjustments off (for comparison)\n",
    "print(\"\\nTesting with dynamic adjustments OFF:\")\n",
    "test_dynamic_training_mirrored(dynamic_adjustments=False, epochs=5)\n",
    "\n",
    "# Visualize resource usage using log_usage_plot\n",
    "log_usage_plot(\"resource_log.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5905cbd8-25dd-4bcd-ad9c-86c57808f4e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "# Log resource usage (regardless of dynamic adjustments)\n",
    "log_usage_once(log_file, batch_size = base_batch_size, num_workers=2, num_epoch=0)  # Default workers are 2 for logging\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # if dynamic_adjustments:\n",
    "    # Adjust resources dynamically based on system usage\n",
    "    cpu_percent, gpu_memory_usage, gpu_memory_total, gpu_percent = sys_resources()\n",
    "    num_workers = adjust_workers(cpu_threshold=80, gpu_threshold=80)  # Adjust workers based on resources\n",
    "    batch_size = adjust_batch_size(cpu_percent, gpu_percent, base_batch_size)  # Adjust batch size\n",
    "    # else:\n",
    "    #     # Keep default batch size and workers fixed\n",
    "    #     num_workers = 2  # Default number of workers\n",
    "    #     batch_size = base_batch_size  # Default batch size\n",
    "    \n",
    "    # print(f\"Using {num_workers} workers and batch size {batch_size}\")\n",
    "    \n",
    "    # Create the MirroredStrategy for distributed training\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    train_images, train_labels = train_dataset['images'], train_dataset['labels']\n",
    "    input_shape = train_images[0].shape\n",
    "    with strategy.scope():\n",
    "        model = create_model_tf(input_shape=input_shape)\n",
    "        model.fit(train_images, train_labels, batch_size=32, epochs=1)  # Train for 1 epoch at a time\n",
    "        model.fit(train_images, train_labels, batch_size=base_batch_size, epochs=1)  # Train for 1 epoch at a time\n",
    "\n",
    "    # Log resource usage for the current epoch\n",
    "    log_usage_once(log_file, batch_size, num_workers, num_epoch=epoch)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ScaleML_example",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
